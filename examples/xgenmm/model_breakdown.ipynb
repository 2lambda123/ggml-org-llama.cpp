{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "from omegaconf import OmegaConfd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_aspect_ratio': 'anyres', 'num_vision_tokens': 128, 'anyres_patch_sampling': True}\n"
     ]
    }
   ],
   "source": [
    "cfg = dict(\n",
    "    model_family = 'kosmos',\n",
    "    lm_path = 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    # vision_encoder_path = 'ViT-H-14-378-quickgelu',\n",
    "    # vision_encoder_pretrained = 'dfn5b',\n",
    "    vision_encoder_path = 'google/siglip-so400m-patch14-384',\n",
    "    vision_encoder_pretrained = 'google',\n",
    "    num_vision_tokens = 128,\n",
    "    image_aspect_ratio = 'anyres',\n",
    "    anyres_patch_sampling = True,\n",
    "    anyres_grids=[[1,2],[2,1],[2,2],[3,1],[1,3]],\n",
    "    ckpt_pth = '/export/share/manli_shu/models/open-flamingo-dev/anyres_ablation_HFSiglip_patch128-kosmos_non_instruct-phi3_4k_instruct_nq128_pre_V3_5-llava_1p6_ocrmathmix_v4-8x8-ckpt2/checkpoint_0.pt',\n",
    ")\n",
    "cfg = OmegaConf.create(cfg)\n",
    "if cfg.model_family in ['kosmos-instruct', 'kosmos', 'llava']:\n",
    "    additional_kwargs = {\n",
    "        \"image_aspect_ratio\": cfg.image_aspect_ratio,\n",
    "        }\n",
    "    if cfg.model_family in ['kosmos-instruct', 'kosmos']:\n",
    "        additional_kwargs.update({\n",
    "            \"num_vision_tokens\": cfg.num_vision_tokens,\n",
    "            \"anyres_patch_sampling\": cfg.anyres_patch_sampling,\n",
    "        })\n",
    "print(additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeb7d9b79894d648f7aa4d27654cd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kosmos model initialized with 3,931,031,619 trainable parameters\n",
      "==========Trainable Parameters\n",
      "Vision encoder: 0 trainable parameters\n",
      "Vision tokenizer: 109,901,568 trainable parameters\n",
      "Language model: 3,821,130,051 trainable parameters\n",
      "==========Total Parameters\n",
      "Vision encoder: 428,225,600 parameters\n",
      "Vision tokenizer: 109,901,568 parameters\n",
      "Language model: 3,821,130,051 parameters\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=cfg.vision_encoder_path,\n",
    "    clip_vision_encoder_pretrained=cfg.vision_encoder_pretrained,\n",
    "    lang_model_path=cfg.lm_path,\n",
    "    tokenizer_path=cfg.lm_path,\n",
    "    model_family=cfg.model_family,\n",
    "    **additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(cfg.ckpt_pth)[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceiverResampler(\n",
       "  (projection): Linear(in_features=1152, out_features=3072, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x ModuleList(\n",
       "      (0): PerceiverAttention(\n",
       "        (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "        (to_kv): Linear(in_features=1152, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1152, out_features=4608, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=4608, out_features=1152, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    Lambda()\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder.config.save_pretrained('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vit_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    Lambda()\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer, SiglipVisionConfig\n",
    "import json\n",
    "import torch\n",
    "with open('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vision_encoder/config.json', 'r') as f:\n",
    "    vision_config = json.load(f)\n",
    "vision_config = SiglipVisionConfig(**vision_config)\n",
    "vision_encoder = SiglipVisionTransformer(vision_config)\n",
    "vit_ckpt = torch.load('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vision_encoder/xgenmm.vision_encoder')\n",
    "vision_encoder.load_state_dict(vit_ckpt, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = vision_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.patch_embedding.weight', 'embeddings.patch_embedding.bias', 'embeddings.position_embedding.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.layer_norm1.weight', 'encoder.layers.0.layer_norm1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.layer_norm2.weight', 'encoder.layers.0.layer_norm2.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.layer_norm1.weight', 'encoder.layers.1.layer_norm1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.layer_norm2.weight', 'encoder.layers.1.layer_norm2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.layer_norm1.weight', 'encoder.layers.2.layer_norm1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.layer_norm2.weight', 'encoder.layers.2.layer_norm2.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.layer_norm1.weight', 'encoder.layers.3.layer_norm1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.layer_norm2.weight', 'encoder.layers.3.layer_norm2.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.layer_norm1.weight', 'encoder.layers.4.layer_norm1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.layer_norm2.weight', 'encoder.layers.4.layer_norm2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.layer_norm1.weight', 'encoder.layers.5.layer_norm1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.layer_norm2.weight', 'encoder.layers.5.layer_norm2.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.layer_norm1.weight', 'encoder.layers.6.layer_norm1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.layer_norm2.weight', 'encoder.layers.6.layer_norm2.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.layer_norm1.weight', 'encoder.layers.7.layer_norm1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.layer_norm2.weight', 'encoder.layers.7.layer_norm2.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.layer_norm1.weight', 'encoder.layers.8.layer_norm1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.layer_norm2.weight', 'encoder.layers.8.layer_norm2.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.layer_norm1.weight', 'encoder.layers.9.layer_norm1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.layer_norm2.weight', 'encoder.layers.9.layer_norm2.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.layer_norm1.weight', 'encoder.layers.10.layer_norm1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.layer_norm2.weight', 'encoder.layers.10.layer_norm2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.layer_norm1.weight', 'encoder.layers.11.layer_norm1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.layer_norm2.weight', 'encoder.layers.11.layer_norm2.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.12.layer_norm1.weight', 'encoder.layers.12.layer_norm1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.layer_norm2.weight', 'encoder.layers.12.layer_norm2.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.13.layer_norm1.weight', 'encoder.layers.13.layer_norm1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.layer_norm2.weight', 'encoder.layers.13.layer_norm2.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.14.layer_norm1.weight', 'encoder.layers.14.layer_norm1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.layer_norm2.weight', 'encoder.layers.14.layer_norm2.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.15.layer_norm1.weight', 'encoder.layers.15.layer_norm1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.layer_norm2.weight', 'encoder.layers.15.layer_norm2.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.16.layer_norm1.weight', 'encoder.layers.16.layer_norm1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.layer_norm2.weight', 'encoder.layers.16.layer_norm2.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.17.layer_norm1.weight', 'encoder.layers.17.layer_norm1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.layer_norm2.weight', 'encoder.layers.17.layer_norm2.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.18.layer_norm1.weight', 'encoder.layers.18.layer_norm1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.layer_norm2.weight', 'encoder.layers.18.layer_norm2.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.19.layer_norm1.weight', 'encoder.layers.19.layer_norm1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.layer_norm2.weight', 'encoder.layers.19.layer_norm2.bias', 'encoder.layers.20.self_attn.k_proj.weight', 'encoder.layers.20.self_attn.k_proj.bias', 'encoder.layers.20.self_attn.v_proj.weight', 'encoder.layers.20.self_attn.v_proj.bias', 'encoder.layers.20.self_attn.q_proj.weight', 'encoder.layers.20.self_attn.q_proj.bias', 'encoder.layers.20.self_attn.out_proj.weight', 'encoder.layers.20.self_attn.out_proj.bias', 'encoder.layers.20.layer_norm1.weight', 'encoder.layers.20.layer_norm1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.layer_norm2.weight', 'encoder.layers.20.layer_norm2.bias', 'encoder.layers.21.self_attn.k_proj.weight', 'encoder.layers.21.self_attn.k_proj.bias', 'encoder.layers.21.self_attn.v_proj.weight', 'encoder.layers.21.self_attn.v_proj.bias', 'encoder.layers.21.self_attn.q_proj.weight', 'encoder.layers.21.self_attn.q_proj.bias', 'encoder.layers.21.self_attn.out_proj.weight', 'encoder.layers.21.self_attn.out_proj.bias', 'encoder.layers.21.layer_norm1.weight', 'encoder.layers.21.layer_norm1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.layer_norm2.weight', 'encoder.layers.21.layer_norm2.bias', 'encoder.layers.22.self_attn.k_proj.weight', 'encoder.layers.22.self_attn.k_proj.bias', 'encoder.layers.22.self_attn.v_proj.weight', 'encoder.layers.22.self_attn.v_proj.bias', 'encoder.layers.22.self_attn.q_proj.weight', 'encoder.layers.22.self_attn.q_proj.bias', 'encoder.layers.22.self_attn.out_proj.weight', 'encoder.layers.22.self_attn.out_proj.bias', 'encoder.layers.22.layer_norm1.weight', 'encoder.layers.22.layer_norm1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.layer_norm2.weight', 'encoder.layers.22.layer_norm2.bias', 'encoder.layers.23.self_attn.k_proj.weight', 'encoder.layers.23.self_attn.k_proj.bias', 'encoder.layers.23.self_attn.v_proj.weight', 'encoder.layers.23.self_attn.v_proj.bias', 'encoder.layers.23.self_attn.q_proj.weight', 'encoder.layers.23.self_attn.q_proj.bias', 'encoder.layers.23.self_attn.out_proj.weight', 'encoder.layers.23.self_attn.out_proj.bias', 'encoder.layers.23.layer_norm1.weight', 'encoder.layers.23.layer_norm1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.layer_norm2.weight', 'encoder.layers.23.layer_norm2.bias', 'encoder.layers.24.self_attn.k_proj.weight', 'encoder.layers.24.self_attn.k_proj.bias', 'encoder.layers.24.self_attn.v_proj.weight', 'encoder.layers.24.self_attn.v_proj.bias', 'encoder.layers.24.self_attn.q_proj.weight', 'encoder.layers.24.self_attn.q_proj.bias', 'encoder.layers.24.self_attn.out_proj.weight', 'encoder.layers.24.self_attn.out_proj.bias', 'encoder.layers.24.layer_norm1.weight', 'encoder.layers.24.layer_norm1.bias', 'encoder.layers.24.mlp.fc1.weight', 'encoder.layers.24.mlp.fc1.bias', 'encoder.layers.24.mlp.fc2.weight', 'encoder.layers.24.mlp.fc2.bias', 'encoder.layers.24.layer_norm2.weight', 'encoder.layers.24.layer_norm2.bias', 'encoder.layers.25.self_attn.k_proj.weight', 'encoder.layers.25.self_attn.k_proj.bias', 'encoder.layers.25.self_attn.v_proj.weight', 'encoder.layers.25.self_attn.v_proj.bias', 'encoder.layers.25.self_attn.q_proj.weight', 'encoder.layers.25.self_attn.q_proj.bias', 'encoder.layers.25.self_attn.out_proj.weight', 'encoder.layers.25.self_attn.out_proj.bias', 'encoder.layers.25.layer_norm1.weight', 'encoder.layers.25.layer_norm1.bias', 'encoder.layers.25.mlp.fc1.weight', 'encoder.layers.25.mlp.fc1.bias', 'encoder.layers.25.mlp.fc2.weight', 'encoder.layers.25.mlp.fc2.bias', 'encoder.layers.25.layer_norm2.weight', 'encoder.layers.25.layer_norm2.bias', 'encoder.layers.26.self_attn.k_proj.weight', 'encoder.layers.26.self_attn.k_proj.bias', 'encoder.layers.26.self_attn.v_proj.weight', 'encoder.layers.26.self_attn.v_proj.bias', 'encoder.layers.26.self_attn.q_proj.weight', 'encoder.layers.26.self_attn.q_proj.bias', 'encoder.layers.26.self_attn.out_proj.weight', 'encoder.layers.26.self_attn.out_proj.bias', 'encoder.layers.26.layer_norm1.weight', 'encoder.layers.26.layer_norm1.bias', 'encoder.layers.26.mlp.fc1.weight', 'encoder.layers.26.mlp.fc1.bias', 'encoder.layers.26.mlp.fc2.weight', 'encoder.layers.26.mlp.fc2.bias', 'encoder.layers.26.layer_norm2.weight', 'encoder.layers.26.layer_norm2.bias', 'post_layernorm.weight', 'post_layernorm.bias', 'head.probe', 'head.attention.in_proj_weight', 'head.attention.in_proj_bias', 'head.attention.out_proj.weight', 'head.attention.out_proj.bias', 'head.layernorm.weight', 'head.layernorm.bias', 'head.mlp.fc1.weight', 'head.mlp.fc1.bias', 'head.mlp.fc2.weight', 'head.mlp.fc2.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 1152])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.position_embedding.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 729, 1152])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.position_embedding.weight'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = torch.load('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/xgenmm.projector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents\n",
      "projection.weight\n",
      "projection.bias\n",
      "layers.0.0.norm_media.weight\n",
      "layers.0.0.norm_media.bias\n",
      "layers.0.0.norm_latents.weight\n",
      "layers.0.0.norm_latents.bias\n",
      "layers.0.0.to_q.weight\n",
      "layers.0.0.to_kv.weight\n",
      "layers.0.0.to_out.weight\n",
      "layers.0.1.0.weight\n",
      "layers.0.1.0.bias\n",
      "layers.0.1.1.weight\n",
      "layers.0.1.3.weight\n",
      "layers.1.0.norm_media.weight\n",
      "layers.1.0.norm_media.bias\n",
      "layers.1.0.norm_latents.weight\n",
      "layers.1.0.norm_latents.bias\n",
      "layers.1.0.to_q.weight\n",
      "layers.1.0.to_kv.weight\n",
      "layers.1.0.to_out.weight\n",
      "layers.1.1.0.weight\n",
      "layers.1.1.0.bias\n",
      "layers.1.1.1.weight\n",
      "layers.1.1.3.weight\n",
      "layers.2.0.norm_media.weight\n",
      "layers.2.0.norm_media.bias\n",
      "layers.2.0.norm_latents.weight\n",
      "layers.2.0.norm_latents.bias\n",
      "layers.2.0.to_q.weight\n",
      "layers.2.0.to_kv.weight\n",
      "layers.2.0.to_out.weight\n",
      "layers.2.1.0.weight\n",
      "layers.2.1.0.bias\n",
      "layers.2.1.1.weight\n",
      "layers.2.1.3.weight\n",
      "layers.3.0.norm_media.weight\n",
      "layers.3.0.norm_media.bias\n",
      "layers.3.0.norm_latents.weight\n",
      "layers.3.0.norm_latents.bias\n",
      "layers.3.0.to_q.weight\n",
      "layers.3.0.to_kv.weight\n",
      "layers.3.0.to_out.weight\n",
      "layers.3.1.0.weight\n",
      "layers.3.1.0.bias\n",
      "layers.3.1.1.weight\n",
      "layers.3.1.3.weight\n",
      "layers.4.0.norm_media.weight\n",
      "layers.4.0.norm_media.bias\n",
      "layers.4.0.norm_latents.weight\n",
      "layers.4.0.norm_latents.bias\n",
      "layers.4.0.to_q.weight\n",
      "layers.4.0.to_kv.weight\n",
      "layers.4.0.to_out.weight\n",
      "layers.4.1.0.weight\n",
      "layers.4.1.0.bias\n",
      "layers.4.1.1.weight\n",
      "layers.4.1.3.weight\n",
      "layers.5.0.norm_media.weight\n",
      "layers.5.0.norm_media.bias\n",
      "layers.5.0.norm_latents.weight\n",
      "layers.5.0.norm_latents.bias\n",
      "layers.5.0.to_q.weight\n",
      "layers.5.0.to_kv.weight\n",
      "layers.5.0.to_out.weight\n",
      "layers.5.1.0.weight\n",
      "layers.5.1.0.bias\n",
      "layers.5.1.1.weight\n",
      "layers.5.1.3.weight\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    }
   ],
   "source": [
    "for k in projector.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_q = projector['layers.0.0.to_q.weight']\n",
    "to_kv= projector['layers.0.0.to_kv.weight']\n",
    "to_o = projector['layers.0.0.to_out.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1152]), torch.Size([3072, 1152]), torch.Size([1152, 1536]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_q.shape, to_kv.shape, to_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attn.q.weight'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(\"attn.in_proj_\", \"attn.q.\", 'attn.in_proj_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "on_device",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
