{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from open_flamingo.train.any_res_data_utils import process_images\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, Lambda\n",
    "from torchvision.transforms import InterpolationMode\n",
    "BICUBIC = InterpolationMode.BICUBIC\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "\n",
    "cfg = dict(\n",
    "    model_family = 'kosmos',\n",
    "    lm_path = 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    # vision_encoder_path = 'ViT-H-14-378-quickgelu',\n",
    "    # vision_encoder_pretrained = 'dfn5b',\n",
    "    vision_encoder_path = 'google/siglip-so400m-patch14-384',\n",
    "    vision_encoder_pretrained = 'google',\n",
    "    num_vision_tokens = 128,\n",
    "    image_aspect_ratio = 'anyres',\n",
    "    anyres_patch_sampling = True,\n",
    "    anyres_grids=[[1,2],[2,1],[2,2],[3,1],[1,3]],\n",
    "    ckpt_pth = '/export/share/manli_shu/models/open-flamingo-dev/anyres_ablation_HFSiglip_patch128-kosmos_non_instruct-phi3_4k_instruct_nq128_pre_V3_5-llava_1p6_ocrmathmix_v4-8x8-ckpt2/checkpoint_0.pt',\n",
    ")\n",
    "cfg = OmegaConf.create(cfg)\n",
    "n_px = 384\n",
    "image_processor = Compose([\n",
    "        Resize((n_px, n_px), interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
    "        Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "image_proc = partial(process_images, image_processor=image_processor, model_cfg=cfg)\n",
    "base_img_size = image_processor.transforms[0].size[0]\n",
    "anyres_grids = []\n",
    "for (m,n) in cfg.anyres_grids:\n",
    "    anyres_grids.append([base_img_size*m, base_img_size*n])\n",
    "cfg.anyres_grids = anyres_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_aspect_ratio: anyres\n",
      "anyres_grids: [[384, 768], [768, 384], [768, 768], [1152, 384], [384, 1152]]\n"
     ]
    }
   ],
   "source": [
    "image_aspect_ratio = cfg.image_aspect_ratio\n",
    "print(f\"image_aspect_ratio: {image_aspect_ratio}\")\n",
    "anyres_grids = cfg.anyres_grids\n",
    "print(f\"anyres_grids: {anyres_grids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./imgs\"\n",
    "image_path_1 = f'{img_dir}/image-1d100e9-1.jpg'\n",
    "image_path_2 = f'{img_dir}/image-1d100e9.jpg'\n",
    "image_1 = Image.open(image_path_1).convert('RGB')\n",
    "image_2 = Image.open(image_path_2).convert('RGB')\n",
    "images = [image_1, image_2]\n",
    "image_size = [image_1.size, image_2.size]\n",
    "image_size = [image_size]\n",
    "vision_x = [image_proc([img]) for img in images]\n",
    "vision_x = [vision_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Resize'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True),\n",
       " Lambda(),\n",
       " ToTensor(),\n",
       " Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(image_processor.transforms[0]))\n",
    "image_processor.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[217, 212, 211, 213, 213, 210, 210, 210, 213, 214],\n",
       "       [213, 211, 212, 212, 209, 212, 211, 210, 210, 211],\n",
       "       [213, 211, 211, 212, 210, 213, 212, 211, 210, 210],\n",
       "       [215, 211, 209, 212, 212, 211, 210, 210, 210, 210],\n",
       "       [211, 208, 209, 211, 210, 211, 211, 211, 211, 211]], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.asarray(image_processor.transforms[0](image_2))[:5, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_interpolate(p, x):\n",
    "    return (\n",
    "        p[1] +\n",
    "        0.5 * x * (p[2] - p[0] + \n",
    "        x * (2.0 * p[0] - 5.0 * p[1] + 4.0 * p[2] - p[3] + \n",
    "        x * (3.0 * (p[1] - p[2]) + p[3] - p[0])))\n",
    "    )\n",
    "\n",
    "def bicubic_interpolate(p, x, y):\n",
    "    arr = np.array([cubic_interpolate(p[i], y) for i in range(4)])\n",
    "    return cubic_interpolate(arr, x)\n",
    "\n",
    "def resize_bicubic_pil(image, new_width, new_height):\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    height, width, channels = image_np.shape\n",
    "    resized_image = np.zeros((new_height, new_width, channels))\n",
    "\n",
    "    x_ratio = width / new_width\n",
    "    y_ratio = height / new_height\n",
    "\n",
    "    for i in range(new_height):\n",
    "        for j in range(new_width):\n",
    "            x = j * x_ratio\n",
    "            y = i * y_ratio\n",
    "\n",
    "            x_int = int(x)\n",
    "            y_int = int(y)\n",
    "\n",
    "            x_diff = x - x_int\n",
    "            y_diff = y - y_int\n",
    "\n",
    "            p = np.zeros((4, 4, channels))\n",
    "\n",
    "            for m in range(-1, 3):\n",
    "                for n in range(-1, 3):\n",
    "                    xm = min(max(x_int + m, 0), width - 1)\n",
    "                    yn = min(max(y_int + n, 0), height - 1)\n",
    "                    p[m + 1, n + 1] = image_np[yn, xm]\n",
    "\n",
    "            for c in range(channels):\n",
    "                resized_image[i, j, c] = bicubic_interpolate(p[:, :, c], x_diff, y_diff)\n",
    "\n",
    "    # Convert the NumPy array back to a PIL image\n",
    "    resized_image = np.clip(resized_image, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[222, 217, 214, 216, 218, 213, 212, 214, 216, 218],\n",
       "       [213, 209, 209, 211, 209, 209, 209, 209, 208, 210],\n",
       "       [212, 210, 211, 212, 209, 213, 212, 209, 209, 210],\n",
       "       [217, 212, 211, 212, 212, 212, 211, 210, 210, 211],\n",
       "       [212, 208, 208, 210, 210, 210, 211, 211, 211, 210]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = resize_bicubic_pil(image_2, base_img_size, base_img_size)\n",
    "np.asarray(res)[:5, :10, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "from omegaconf import OmegaConfd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_aspect_ratio': 'anyres', 'num_vision_tokens': 128, 'anyres_patch_sampling': True}\n"
     ]
    }
   ],
   "source": [
    "cfg = dict(\n",
    "    model_family = 'kosmos',\n",
    "    lm_path = 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    # vision_encoder_path = 'ViT-H-14-378-quickgelu',\n",
    "    # vision_encoder_pretrained = 'dfn5b',\n",
    "    vision_encoder_path = 'google/siglip-so400m-patch14-384',\n",
    "    vision_encoder_pretrained = 'google',\n",
    "    num_vision_tokens = 128,\n",
    "    image_aspect_ratio = 'anyres',\n",
    "    anyres_patch_sampling = True,\n",
    "    anyres_grids=[[1,2],[2,1],[2,2],[3,1],[1,3]],\n",
    "    ckpt_pth = '/export/share/manli_shu/models/open-flamingo-dev/anyres_ablation_HFSiglip_patch128-kosmos_non_instruct-phi3_4k_instruct_nq128_pre_V3_5-llava_1p6_ocrmathmix_v4-8x8-ckpt2/checkpoint_0.pt',\n",
    ")\n",
    "cfg = OmegaConf.create(cfg)\n",
    "if cfg.model_family in ['kosmos-instruct', 'kosmos', 'llava']:\n",
    "    additional_kwargs = {\n",
    "        \"image_aspect_ratio\": cfg.image_aspect_ratio,\n",
    "        }\n",
    "    if cfg.model_family in ['kosmos-instruct', 'kosmos']:\n",
    "        additional_kwargs.update({\n",
    "            \"num_vision_tokens\": cfg.num_vision_tokens,\n",
    "            \"anyres_patch_sampling\": cfg.anyres_patch_sampling,\n",
    "        })\n",
    "print(additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeb7d9b79894d648f7aa4d27654cd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kosmos model initialized with 3,931,031,619 trainable parameters\n",
      "==========Trainable Parameters\n",
      "Vision encoder: 0 trainable parameters\n",
      "Vision tokenizer: 109,901,568 trainable parameters\n",
      "Language model: 3,821,130,051 trainable parameters\n",
      "==========Total Parameters\n",
      "Vision encoder: 428,225,600 parameters\n",
      "Vision tokenizer: 109,901,568 parameters\n",
      "Language model: 3,821,130,051 parameters\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=cfg.vision_encoder_path,\n",
    "    clip_vision_encoder_pretrained=cfg.vision_encoder_pretrained,\n",
    "    lang_model_path=cfg.lm_path,\n",
    "    tokenizer_path=cfg.lm_path,\n",
    "    model_family=cfg.model_family,\n",
    "    **additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(cfg.ckpt_pth)[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceiverResampler(\n",
       "  (projection): Linear(in_features=1152, out_features=3072, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x ModuleList(\n",
       "      (0): PerceiverAttention(\n",
       "        (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "        (to_kv): Linear(in_features=1152, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1152, out_features=4608, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=4608, out_features=1152, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    Lambda()\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder.config.save_pretrained('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vit_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    Lambda()\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer, SiglipVisionConfig\n",
    "import json\n",
    "import torch\n",
    "with open('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vision_encoder/config.json', 'r') as f:\n",
    "    vision_config = json.load(f)\n",
    "vision_config = SiglipVisionConfig(**vision_config)\n",
    "vision_encoder = SiglipVisionTransformer(vision_config)\n",
    "vit_ckpt = torch.load('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/vision_encoder/xgenmm.vision_encoder')\n",
    "vision_encoder.load_state_dict(vit_ckpt, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = vision_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.patch_embedding.weight', 'embeddings.patch_embedding.bias', 'embeddings.position_embedding.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.layer_norm1.weight', 'encoder.layers.0.layer_norm1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.layer_norm2.weight', 'encoder.layers.0.layer_norm2.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.layer_norm1.weight', 'encoder.layers.1.layer_norm1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.layer_norm2.weight', 'encoder.layers.1.layer_norm2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.layer_norm1.weight', 'encoder.layers.2.layer_norm1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.layer_norm2.weight', 'encoder.layers.2.layer_norm2.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.layer_norm1.weight', 'encoder.layers.3.layer_norm1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.layer_norm2.weight', 'encoder.layers.3.layer_norm2.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.layer_norm1.weight', 'encoder.layers.4.layer_norm1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.layer_norm2.weight', 'encoder.layers.4.layer_norm2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.layer_norm1.weight', 'encoder.layers.5.layer_norm1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.layer_norm2.weight', 'encoder.layers.5.layer_norm2.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.layer_norm1.weight', 'encoder.layers.6.layer_norm1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.layer_norm2.weight', 'encoder.layers.6.layer_norm2.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.layer_norm1.weight', 'encoder.layers.7.layer_norm1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.layer_norm2.weight', 'encoder.layers.7.layer_norm2.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.layer_norm1.weight', 'encoder.layers.8.layer_norm1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.layer_norm2.weight', 'encoder.layers.8.layer_norm2.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.layer_norm1.weight', 'encoder.layers.9.layer_norm1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.layer_norm2.weight', 'encoder.layers.9.layer_norm2.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.layer_norm1.weight', 'encoder.layers.10.layer_norm1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.layer_norm2.weight', 'encoder.layers.10.layer_norm2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.layer_norm1.weight', 'encoder.layers.11.layer_norm1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.layer_norm2.weight', 'encoder.layers.11.layer_norm2.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.12.layer_norm1.weight', 'encoder.layers.12.layer_norm1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.layer_norm2.weight', 'encoder.layers.12.layer_norm2.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.13.layer_norm1.weight', 'encoder.layers.13.layer_norm1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.layer_norm2.weight', 'encoder.layers.13.layer_norm2.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.14.layer_norm1.weight', 'encoder.layers.14.layer_norm1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.layer_norm2.weight', 'encoder.layers.14.layer_norm2.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.15.layer_norm1.weight', 'encoder.layers.15.layer_norm1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.layer_norm2.weight', 'encoder.layers.15.layer_norm2.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.16.layer_norm1.weight', 'encoder.layers.16.layer_norm1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.layer_norm2.weight', 'encoder.layers.16.layer_norm2.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.17.layer_norm1.weight', 'encoder.layers.17.layer_norm1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.layer_norm2.weight', 'encoder.layers.17.layer_norm2.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.18.layer_norm1.weight', 'encoder.layers.18.layer_norm1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.layer_norm2.weight', 'encoder.layers.18.layer_norm2.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.19.layer_norm1.weight', 'encoder.layers.19.layer_norm1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.layer_norm2.weight', 'encoder.layers.19.layer_norm2.bias', 'encoder.layers.20.self_attn.k_proj.weight', 'encoder.layers.20.self_attn.k_proj.bias', 'encoder.layers.20.self_attn.v_proj.weight', 'encoder.layers.20.self_attn.v_proj.bias', 'encoder.layers.20.self_attn.q_proj.weight', 'encoder.layers.20.self_attn.q_proj.bias', 'encoder.layers.20.self_attn.out_proj.weight', 'encoder.layers.20.self_attn.out_proj.bias', 'encoder.layers.20.layer_norm1.weight', 'encoder.layers.20.layer_norm1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.layer_norm2.weight', 'encoder.layers.20.layer_norm2.bias', 'encoder.layers.21.self_attn.k_proj.weight', 'encoder.layers.21.self_attn.k_proj.bias', 'encoder.layers.21.self_attn.v_proj.weight', 'encoder.layers.21.self_attn.v_proj.bias', 'encoder.layers.21.self_attn.q_proj.weight', 'encoder.layers.21.self_attn.q_proj.bias', 'encoder.layers.21.self_attn.out_proj.weight', 'encoder.layers.21.self_attn.out_proj.bias', 'encoder.layers.21.layer_norm1.weight', 'encoder.layers.21.layer_norm1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.layer_norm2.weight', 'encoder.layers.21.layer_norm2.bias', 'encoder.layers.22.self_attn.k_proj.weight', 'encoder.layers.22.self_attn.k_proj.bias', 'encoder.layers.22.self_attn.v_proj.weight', 'encoder.layers.22.self_attn.v_proj.bias', 'encoder.layers.22.self_attn.q_proj.weight', 'encoder.layers.22.self_attn.q_proj.bias', 'encoder.layers.22.self_attn.out_proj.weight', 'encoder.layers.22.self_attn.out_proj.bias', 'encoder.layers.22.layer_norm1.weight', 'encoder.layers.22.layer_norm1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.layer_norm2.weight', 'encoder.layers.22.layer_norm2.bias', 'encoder.layers.23.self_attn.k_proj.weight', 'encoder.layers.23.self_attn.k_proj.bias', 'encoder.layers.23.self_attn.v_proj.weight', 'encoder.layers.23.self_attn.v_proj.bias', 'encoder.layers.23.self_attn.q_proj.weight', 'encoder.layers.23.self_attn.q_proj.bias', 'encoder.layers.23.self_attn.out_proj.weight', 'encoder.layers.23.self_attn.out_proj.bias', 'encoder.layers.23.layer_norm1.weight', 'encoder.layers.23.layer_norm1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.layer_norm2.weight', 'encoder.layers.23.layer_norm2.bias', 'encoder.layers.24.self_attn.k_proj.weight', 'encoder.layers.24.self_attn.k_proj.bias', 'encoder.layers.24.self_attn.v_proj.weight', 'encoder.layers.24.self_attn.v_proj.bias', 'encoder.layers.24.self_attn.q_proj.weight', 'encoder.layers.24.self_attn.q_proj.bias', 'encoder.layers.24.self_attn.out_proj.weight', 'encoder.layers.24.self_attn.out_proj.bias', 'encoder.layers.24.layer_norm1.weight', 'encoder.layers.24.layer_norm1.bias', 'encoder.layers.24.mlp.fc1.weight', 'encoder.layers.24.mlp.fc1.bias', 'encoder.layers.24.mlp.fc2.weight', 'encoder.layers.24.mlp.fc2.bias', 'encoder.layers.24.layer_norm2.weight', 'encoder.layers.24.layer_norm2.bias', 'encoder.layers.25.self_attn.k_proj.weight', 'encoder.layers.25.self_attn.k_proj.bias', 'encoder.layers.25.self_attn.v_proj.weight', 'encoder.layers.25.self_attn.v_proj.bias', 'encoder.layers.25.self_attn.q_proj.weight', 'encoder.layers.25.self_attn.q_proj.bias', 'encoder.layers.25.self_attn.out_proj.weight', 'encoder.layers.25.self_attn.out_proj.bias', 'encoder.layers.25.layer_norm1.weight', 'encoder.layers.25.layer_norm1.bias', 'encoder.layers.25.mlp.fc1.weight', 'encoder.layers.25.mlp.fc1.bias', 'encoder.layers.25.mlp.fc2.weight', 'encoder.layers.25.mlp.fc2.bias', 'encoder.layers.25.layer_norm2.weight', 'encoder.layers.25.layer_norm2.bias', 'encoder.layers.26.self_attn.k_proj.weight', 'encoder.layers.26.self_attn.k_proj.bias', 'encoder.layers.26.self_attn.v_proj.weight', 'encoder.layers.26.self_attn.v_proj.bias', 'encoder.layers.26.self_attn.q_proj.weight', 'encoder.layers.26.self_attn.q_proj.bias', 'encoder.layers.26.self_attn.out_proj.weight', 'encoder.layers.26.self_attn.out_proj.bias', 'encoder.layers.26.layer_norm1.weight', 'encoder.layers.26.layer_norm1.bias', 'encoder.layers.26.mlp.fc1.weight', 'encoder.layers.26.mlp.fc1.bias', 'encoder.layers.26.mlp.fc2.weight', 'encoder.layers.26.mlp.fc2.bias', 'encoder.layers.26.layer_norm2.weight', 'encoder.layers.26.layer_norm2.bias', 'post_layernorm.weight', 'post_layernorm.bias', 'head.probe', 'head.attention.in_proj_weight', 'head.attention.in_proj_bias', 'head.attention.out_proj.weight', 'head.attention.out_proj.bias', 'head.layernorm.weight', 'head.layernorm.bias', 'head.mlp.fc1.weight', 'head.mlp.fc1.bias', 'head.mlp.fc2.weight', 'head.mlp.fc2.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 1152])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.position_embedding.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 729, 1152])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.position_embedding.weight'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = torch.load('/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct/xgenmm.projector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents\n",
      "projection.weight\n",
      "projection.bias\n",
      "layers.0.0.norm_media.weight\n",
      "layers.0.0.norm_media.bias\n",
      "layers.0.0.norm_latents.weight\n",
      "layers.0.0.norm_latents.bias\n",
      "layers.0.0.to_q.weight\n",
      "layers.0.0.to_kv.weight\n",
      "layers.0.0.to_out.weight\n",
      "layers.0.1.0.weight\n",
      "layers.0.1.0.bias\n",
      "layers.0.1.1.weight\n",
      "layers.0.1.3.weight\n",
      "layers.1.0.norm_media.weight\n",
      "layers.1.0.norm_media.bias\n",
      "layers.1.0.norm_latents.weight\n",
      "layers.1.0.norm_latents.bias\n",
      "layers.1.0.to_q.weight\n",
      "layers.1.0.to_kv.weight\n",
      "layers.1.0.to_out.weight\n",
      "layers.1.1.0.weight\n",
      "layers.1.1.0.bias\n",
      "layers.1.1.1.weight\n",
      "layers.1.1.3.weight\n",
      "layers.2.0.norm_media.weight\n",
      "layers.2.0.norm_media.bias\n",
      "layers.2.0.norm_latents.weight\n",
      "layers.2.0.norm_latents.bias\n",
      "layers.2.0.to_q.weight\n",
      "layers.2.0.to_kv.weight\n",
      "layers.2.0.to_out.weight\n",
      "layers.2.1.0.weight\n",
      "layers.2.1.0.bias\n",
      "layers.2.1.1.weight\n",
      "layers.2.1.3.weight\n",
      "layers.3.0.norm_media.weight\n",
      "layers.3.0.norm_media.bias\n",
      "layers.3.0.norm_latents.weight\n",
      "layers.3.0.norm_latents.bias\n",
      "layers.3.0.to_q.weight\n",
      "layers.3.0.to_kv.weight\n",
      "layers.3.0.to_out.weight\n",
      "layers.3.1.0.weight\n",
      "layers.3.1.0.bias\n",
      "layers.3.1.1.weight\n",
      "layers.3.1.3.weight\n",
      "layers.4.0.norm_media.weight\n",
      "layers.4.0.norm_media.bias\n",
      "layers.4.0.norm_latents.weight\n",
      "layers.4.0.norm_latents.bias\n",
      "layers.4.0.to_q.weight\n",
      "layers.4.0.to_kv.weight\n",
      "layers.4.0.to_out.weight\n",
      "layers.4.1.0.weight\n",
      "layers.4.1.0.bias\n",
      "layers.4.1.1.weight\n",
      "layers.4.1.3.weight\n",
      "layers.5.0.norm_media.weight\n",
      "layers.5.0.norm_media.bias\n",
      "layers.5.0.norm_latents.weight\n",
      "layers.5.0.norm_latents.bias\n",
      "layers.5.0.to_q.weight\n",
      "layers.5.0.to_kv.weight\n",
      "layers.5.0.to_out.weight\n",
      "layers.5.1.0.weight\n",
      "layers.5.1.0.bias\n",
      "layers.5.1.1.weight\n",
      "layers.5.1.3.weight\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    }
   ],
   "source": [
    "for k in projector.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_q = projector['layers.0.0.to_q.weight']\n",
    "to_kv= projector['layers.0.0.to_kv.weight']\n",
    "to_o = projector['layers.0.0.to_out.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1152]), torch.Size([3072, 1152]), torch.Size([1152, 1536]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_q.shape, to_kv.shape, to_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attn.q.weight'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(\"attn.in_proj_\", \"attn.q.\", 'attn.in_proj_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "on_device",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
